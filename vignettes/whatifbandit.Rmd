---
title: "Introduction to whatifbandit"
output: rmarkdown::html_vignette
bibliography: "REFERENCES.bib"
cache: TRUE
vignette: >
  %\VignetteIndexEntry{Introduction to whatifbandit}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
library(data.table)
library(dplyr)
library(future)
library(ggplot2)
library(tidyr)
library(whatifbandit)

```



whatifbandit allows researchers to re-simulate their randomized experiments under adaptive experimental conditions. Running experiments can be costly and time-consuming, so this simulation tool allows researchers to explore what their results could have looked like under an adaptive experimental design. These re-analyses can inform the planning for future studies, allowing a researcher to justify an adaptive design for a future project, before running a costly experiment.

Adaptive experimental designs tend to shift more participants over time to more promising treatment arms, which are more efficient at honing in on the best treatment. These designs can shine in situations such as:

-   Simple random assignment produces sample sizes too small to detect treatment effects.
-   A researcher wants to test a many treatments.
-   An experiment occurs over a long period of time.
-   Finding the absolute best treatment takes precedence over gauging the effects of all treatments.

This vignette introduces the key functions of the whatifbandit package and how they to use them effectively. All that's required is data from a previous randomized experiment containing information about which treatment each observation was originally assigned too, and the original outcome. For a more detailed view of the exact adaptive procedures, and implementation, please refer to the function documentation instead.

## Multi-Arm-Bandits: Adaptive Assignment

Adaptive experiments are a form of Multi-Arm-Bandit problem, where each treatment arm has an unknown probability of success, and a balance must be achieved between exploration and exploitation. In an applied setting, like an experiment, we want a design that explores the space of possible treatment options to find the best treatment, then exploit that best treatment. If we do too much exploitation, we have the chance to miss the true best treatment, but if we do too much exploration, instead of narrowing down the best treatments, then we miss out on the rewards from that treatment. Additionally, these unknown probabilities can change overtime in what is called a non-stationary bandit problem. The precise details of this problem is not important, but solutions to these bandit problems, in the form of decision algorithms, provide the basis for how whatifbandit simulates an adaptive trial.

whatifbandit supports 2 decision algorithms, Thompson Sampling and UCB1. Thompson Sampling calculates the probability of each treatment being the best treatment arm, then we probability match to assign the treatment arms. In the Bernoulli case, which whatifbandit uses, this can be calculated from a beta distribution. UCB1 instead calculates an upper confidence bound on the expected reward of each arm, and selects the maximum value [@kuleshov2014; @slivkins2024; @offer-westort2021]. A full treatment of these issues, is beyond the scope of this vignette, but this introduction hopefully allowed you to understand what is going on behind the whatifbandit simulation.


$$
\hat{\theta}^k_i=\mathbb{E}[\text{Beta}(1 + \text{successes}_i, 1 + \text{failures}_i)] \tag{Thompson}
$$
$$
\arg\max_i  \left(\mathbb{E}[\text{Reward}_i] + \sqrt{\frac{2 \ln t}{n_i}} \right) \tag{UCB1}
$$

## Adaptive Inference

Statistical inference under adaptive designs requires careful consideration. The sample mean cannot be used, $\left(\bar{x} = \frac{1}{n}\sum_i^n x_i \right)$, because observations are no longer independently identically distributed(i.i.d.) as we have not assigned randomly. Instead we utilize an augmented inverse probability weighted estimator (AIPW), with a constant allocation of the variance across periods formulated by Hadad et. al (2021), whose formulas we use in the package. These estimators are unbiased and asymptotically normal under adaptive trials, so a normal distribution can be used by inference. Additionally, whatifbandit also allows for multiple simulations to easily estimate the variance of the procedure, and build an empirical distribution of the unbiased AIPW estimates, instead of just relying on the normal approximation. 

$$ 
\widehat{\Gamma}^{AIPW}_t = \frac{\mathbb{I}\{W_t = w\}}{e_t(w)} Y_t + \left(1 - \frac{\mathbb{I}\{W_t = w\}}{e_t(w)} \right)\hat{m}(w) \tag {Individual} \\
$$

$$
\widehat{Q}^h_t(W) = \frac{\sum_{t=1}^Th_t(w)\widehat{\Gamma}^{AIPW}_t}{\sum_{t=1}^Th_t(w)} \tag{Average}
$$
$$
h_t(w) = \sqrt{\frac{e_t}{T}} \tag{Adaptive Weight} \\
$$
$$
e_t(w) = Pr(W_t = w) \tag{Probability}
$$
$$
\hat{m}(w) = \frac{1}{t - 1} \sum_{s=1}^{t-1} \left( \frac{1}{n(w)_s} \sum_{i=1}^{n(w)_s} x_i(w) \right) \tag{Grouped Mean}
$$
The estimator is calculated by weighting the outcome against the probability of being assigned the treatment, and a conditional expectation/regression estimate based on the previous periods of assignment, then aggregating based on an adaptive weight. In the package the condition expectation is estimated via a grouped mean, and the adaptive weight uses the constant allocation rate described in the paper [@hadad2021a].

## Data

To explore the functionality of whatifbandit, we will use the tanf dataset that comes with the package. This data contains the results of a randomized experiment, which tested the impact of different letter/notices on re-certification for the Temporary Assistance for Needy Families (TANF) program in Washington D.C. [@moore2022]. The original experiment tests 3 different conditions, over the course of 5 months.

```{r data}
data(tanf)
glimpse(tanf)
```

## Core Functions

whatifbandit provides 2 main functions for conducting simulations, `single_mab_simulation()` and `multiple_mab_simulation()`. Like their names suggest `single_mab_simulation()` runs only 1 simulation while `multiple_mab_simulation()` runs a user specified number of simulations. Internally both functions are similar, and share the same arguments except for the few additional ones `multiple_mab_simulation()` has to control the repeated trials. 

## Single Simulations

`single_mab_simulation()` is a catch-all function for running the adaptive simulations, where each of its arguments tunes the settings of the simulation. All that's required for a simulation is a data.frame, containing a unique ID for every row, that row's original treatment, and their original outcome. These column names are passed to the function as strings. 

```{r}
#| eval: FALSE
sim <- single_mab_simulation(data = tanf,
                      assignment_method = "Batch",
                      period_length = 1000,
                      algorithm = "UCB1",
                      conditions = levels(tanf$condition),
                      whole_experiment = TRUE, perfect_assignment = TRUE,
                      prior_periods = "All",
                      blocking = FALSE, 
                      data_cols = c(id_col = "ic_case_id",
                                    success_col = "success",
                                    condition_col = "condition")
                      ) 
```

Additionally, the function contains several options which can be used to tweak the simulation to the specification of the experiment. This current setup performs UCB1 sampling without treatment blocking with batch sizes of 1000 people, where the whole experiment is used for imputing outcomes, which is nothing like the conditions under the TANF experiment. 

The TANF experiment was blocked by D.C. Community and Human Services service centers, ensuring each center had a comparable number of participants in each group. To replicate this we set `block = TRUE` and add the service_center column to `block_cols` [@moore2022].

The original experiment took place over 5 distinct months. To mirror that, we set `assignment_method = "Date"`, `time_unit = "Month"`, and `period_length = 1`, which creates 1 month long periods.  Keep in mind that now `date_col` needs to be provided in `data_cols`. 

UCB1 sampling also only chooses 1 treatment per period, but because our periods time based now and each period is a large portion of the dataset, we should use Thompson Sampling probability matching instead. This ensures that we can see the treatment assignments evolve more over a time. Set `algorithm = "Thompson"` for this. 

The time component of the experiment must be taken into account. There is no guarantee, that by the time next month's letters are sent out, that all the people who would re certify had done so. Additionally, when `whole_experiment = TRUE`, the full set of original results is used for outcome imputation, while setting it to FALSE more only uses the data up to the period, which can be a closer replication. To reflect these items, set `perfect_assignment = FALSE`, and `whole_experiment = FALSE`, along with including `success_date_col` and `assignment_date_col` in `data_cols`. 

With this configuration, if a success occurs after the date treatments are assigned, it is masked and treated as a failure. This simulates the researcher's limited information as those late outcomes would not have been observed yet.

```{r}

sim <- single_mab_simulation(data = tanf,
                      assignment_method = "Date",
                      time_unit = "Month",
                      period_length = 1,
                      algorithm = "Thompson",
                      conditions = levels(tanf$condition),
                      whole_experiment = FALSE, perfect_assignment = TRUE,
                      prior_periods = "All",
                      blocking = TRUE, block_cols = c("service_center"),
                      data_cols = c(id_col = "ic_case_id",
                                    date_col = "appt_date",
                                    success_col = "success",
                                    condition_col = "condition",
                                    month_col = "recert_month",
                                    success_date_col = "date_of_recert",
                                    assignment_date_col = "letter_sent_date")
                      )
```

You don't always have to replicate the setting perfectly, reanalyzing the experiment as an adaptive trial also involves testing how the results change across different configurations, such as the size and length of periods, or the usage of `perfect_assignment` or `whole_experiment`. The only guidelines would be to use UCB1 only when the period size is a relatively small component of the dataset, and `perfect_assignment = FALSE`, when you have dates that line up the periods you have set. If this is not the case, that all the successes will simply be masked, producing strange results.

For example, we can experiment with using individual assignment, by setting `assignment_method = "Individual"`. This does not follow the original experiment, and we don't have data in that form, so `perfect_assignment = TRUE` should be set for the best results. Additionally, since the date still represents the order, I need to sort my data before running the simulation, to ensure its in the proper order.

Its also a strong assumption, that the true probabilities of success for each treatment are stationary over time. To account for this, I can set `prior_periods = 500`, to only look at the last 500 periods, in this case observations, in the simulation when performing adaptive assignment. This ensures that the oldest periods, which might not be representative of the true probabilities, are not considered when assigning new treatments. 

Additionally, because the goal of this experiment is also to estimate a treatment effect for the letters, we might not want to leave 100% of the assignment up to the bandit algorithms. For this we have two methods, either control augmentation or a hybrid approach. 

Control augmentation ensures that the marked control group meets the specified probability threshold for assignment, and if not adjusts accordingly. For example, if I set `control_augment = 0.25` then I'm allocating 25% of each period to be assigned to the control group, and if a period has the control group at 15%, then the probabilities are changed accordingly. 

The hybrid approach sets aside a percentage of the dataset to be assigned randomly, so if I set `random_assign_prop = 0.2`, then I'm allocating 20% of each period to be assigned to treatments randomly, while 80% are assigned via the bandit algorithms.

Whichever option to choose is up to you, but its recommended not to use them together. If there are 3 conditions, like in the TANF experiment, and you set `control_augment = 0.1`, and `random_assign_prop = 0.21`, then the minimum assignment probability for the control group is not 10% but 14.9% instead (0.1 * 0.79 + 0.21/3 = 0.149). Here lets use  `control_augment = 0.2`, and because of this I need to mark the control group by naming the `conditions` character vector, the other names don't matter, they just cannot be NA or NULL. If I chose to use `random_assign_prob` instead the conditions vector can remain unnamed. 

```{r}
#| eval: FALSE
tanf <- arrange(tanf, appt_date)
conditions <- setNames(levels(tanf$condition), c("Control", "T1", "T2"))

sim <- single_mab_simulation(data = tanf,
                      assignment_method = "Individual",
                      algorithm = "Thompson",
                      conditions = conditions,
                      whole_experiment = FALSE, perfect_assignment = TRUE,
                      prior_periods = 500,
                      blocking = TRUE, block_cols = c("service_center"),
                      data_cols = c(id_col = "ic_case_id",
                                    success_col = "success",
                                    condition_col = "condition"),
                      control_augment = 0.2
                      )
```
### Analysis
`single_mab_simulation()` returns a condensed list with information pertinent to the analysis in 4 components:

* `final_data` is the transformed input containing the new assignments, new successes, and other new columns such as the 
probability being assigned each treatment at each period, and the regression adjustment $\hat{m}(W)$ for the AIPW estimates.
* `bandits` contains either the Thompson Sampling or UCB1 statistics from each period in the trial, the chart should be interpreted as the bandit calculation after that period occurred.
* `assignment_probs` contains the probability of assignment from each period in the trial, the chart should be interpreted as the probability for that specific period.
* `estimates` contains the AIPW and Sample estimates with variances for each treatment arm
* `settings` contains the configuration for the simulation.

This object is also a custom `mab` class, that comes with several generic functions to speed up analysis. A `print()` method is provided which displays the settings of the trial, and a `summary()` method which quickly aggregates the results.

```{r}
class(sim)
sim
```
The summary method presents the final Thompson/UCB1 calculation from after the trial concluded, the AIPW, its standard error and a 95% two tailed confidence interval based on a normal distribution.

```{r}
sim_summary <- summary(sim)
print(sim_summary, width = Inf)
```
The width of the confidence intervals can be changed in the `level` argument of the summary method, but can also be calculated by hand using the standard error. This time, lets specify 80% confidence intervals or an $\alpha$ = 0.2.
```{r}
# Inside Summary Call
summary(sim, level = 0.8) |> select(AIPW, SE, lower_bound, upper_bound, level) |> print(width = Inf)
# By hand
quantile <- qnorm(0.2/2, lower.tail = FALSE)
sim_summary |>
  mutate(lower_bound = AIPW - SE*quantile,
         upper_bound = AIPW + SE*quantile) |>
  select(AIPW, SE, lower_bound, upper_bound)


```
#### Plotting
You can also quickly create plots of the results using the plot method. This method uses ggplot2 to create visualizations to the package must be installed. `mab` supports 3 plot types, `"arm"`, `"assign"` and `"estimate"`, each providing different information about the results. These plots can be added to with `+` like any ggplot2 object, and arguments to the main `geom*` can be specified through the `...` argument. The geoms used are `geom_line()`, and `geom_errorbarh()`

`type = "arm"` provides a view of each arm overtime, its relative probability of being the best, or UCB1 statistic.

```{r}
#| fig.width: 5
plot(sim, type = "arm")
```

`type = "assign"` showcases the assignment probability for that arm overtime.

```{r}
#| fig.width: 5
plot(sim, type = "assign")
```

`type = "estimate"` shows error bars on the AIPW or sample estimates using the associated variances. Just like the `summary()`, the confidence level can be specified.

```{r}
#| fig.width: 5
plot(sim, type = "estimate", estimator = "AIPW", level = 0.9, height = 0.4) + 
  scale_x_continuous(breaks = seq(0, 1, .1), limits = range(0, 1))
```

All of the data used to create these plots and summary statistics is present in the original output, so you can easily create your own more detailed versions. Additionally, the information provided should also let you calculate other adaptive estimators, if you do not want to use the AIPW from Hadad et. al (2021) or want to specify another one of their proposed adaptive variance allocation methods.

### Error Handling
`single_mab_simulation()` can have up to 20 arguments, so its easy to make a typo when calling the function. As a result, we provide helpful error messages to help you deduce what has gone wrong with your execution of the function. If the a required column is not properly entered, or any of the arguments or any of the arguments are invalid, a helpful error will be thrown to notify the user of any issues. 

Here the condition column is missing:

```{r}
try(single_mab_simulation(data = tanf,
                      assignment_method = "Batch",
                      period_length = 1000,
                      algorithm = "Thompson",
                      conditions = levels(tanf$condition),
                      whole_experiment = FALSE, perfect_assignment = TRUE,
                      prior_periods = "All",
                      blocking = FALSE, 
                      data_cols = c(id_col = "ic_case_id",
                                    success_col = "success")
                      ))
```
While here `control_augment > 1`:

```{r}
try(single_mab_simulation(data = tanf,
                      assignment_method = "Batch",
                      period_length = 1000,
                      algorithm = "Thompson",
                      conditions = levels(tanf$condition),
                      whole_experiment = FALSE, perfect_assignment = TRUE,
                      prior_periods = "All",
                      blocking = FALSE, 
                      data_cols = c(id_col = "ic_case_id",
                                    success_col = "success",
                                    condition_col = "condition"),
                      control_augment = 1.5
                      
                      ))
```

However the function won't catch if the columns or other arguments are swapped, but are still valid. Be careful when typing, as an error resulting from this mistake will be harder to deduce based on the message.

Here the `success_col` is specified to an existing numeric column, but it does not contain the required information:

```{r}
try(single_mab_simulation(data = tanf,
                      assignment_method = "Batch",
                      period_length = 1000,
                      algorithm = "Thompson",
                      conditions = levels(tanf$condition),
                      whole_experiment = FALSE, perfect_assignment = TRUE,
                      prior_periods = "All",
                      blocking = FALSE, 
                      data_cols = c(id_col = "ic_case_id",
                                    success_col = "recert_id",
                                    condition_col = "condition")
                      ))
```
## Multiple Simulations

`multiple_mab_simulation()` is a shorthand function for executing multiple trials under the same settings. It is used to estimate the variance in the outcomes that occurs from the simulation process. Depending on the dataset size and the number of trials,

## Additional Features



## References {.appendix}
